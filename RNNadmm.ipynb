{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"ckend\" on line 1 in\n",
      "/Users/sichenglei/.matplotlib/matplotlibrc.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0., x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"Date Time\"', '\"p (mbar)\"', '\"T (degC)\"', '\"Tpot (K)\"', '\"Tdew (degC)\"', '\"rh (%)\"', '\"VPmax (mbar)\"', '\"VPact (mbar)\"', '\"VPdef (mbar)\"', '\"sh (g/kg)\"', '\"H2OC (mmol/mol)\"', '\"rho (g/m**3)\"', '\"wv (m/s)\"', '\"max. wv (m/s)\"', '\"wd (deg)\"']\n",
      "420551\n"
     ]
    }
   ],
   "source": [
    "#weather forecasting data\n",
    "import os\n",
    "\n",
    "fname = \"jena_climate_2009_2016.csv\"\n",
    "\n",
    "f = open(fname)\n",
    "data = f.read()\n",
    "f.close()\n",
    "\n",
    "lines = data.split('\\n')\n",
    "header = lines[0].split(',')\n",
    "lines = lines[1:]\n",
    "\n",
    "print (header)\n",
    "print (len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parsing the data, store in an array\n",
    "import numpy as np\n",
    "\n",
    "#data/time dun need to store\n",
    "float_data = np.zeros((len(lines), len(header)-1))\n",
    "for i, line in enumerate(lines):\n",
    "    values = [float(x) for x in line.split(',')[1:]]\n",
    "    float_data[i, :] = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(420551, 14)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "problem formulation:\n",
    "given data going as far back as lookback timesteps (a timestep is 10 minutes)\n",
    "and sampled every steps timesteps,\n",
    "predict the temperature in delay timesteps\n",
    "'''\n",
    "#normalize the data\n",
    "#use the first 200000 data only\n",
    "mean = float_data[:200000].mean(axis=0)\n",
    "float_data -= mean\n",
    "std = float_data[:200000].std(axis=0)\n",
    "float_data /= std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#samples close together are very simialr (redundant)\n",
    "#so we only take one data from each hour\n",
    "def generator(data, lookback, delay, min_index, max_index,\n",
    "             shuffle=False, batch_size=128, step=6):\n",
    "    if max_index is None:\n",
    "        max_index = len(data) - 1 - delay \n",
    "    i = min_index + lookback\n",
    "    while 1:\n",
    "        if shuffle:\n",
    "            rows = np.random.randint(\n",
    "                min_index + lookback, max_index, size=batch_size)\n",
    "        #if not shuffle, then draw data in chronological order\n",
    "        else:\n",
    "            if i + batch_size >= max_index:\n",
    "                #not enough for a new batch, restart\n",
    "                i = min_index + lookback\n",
    "            rows = np.arange(i, min(i+batch_size, max_index))\n",
    "            i += len(rows)\n",
    "        \n",
    "        samples = np.zeros((len(rows), #==batch_size\n",
    "                            lookback//step,\n",
    "                            data.shape[-1]))\n",
    "        targets = np.zeros((len(rows),))\n",
    "        for j, row in enumerate(rows):\n",
    "            indices = range(row-lookback, row, step)\n",
    "            samples[j] = data[indices]\n",
    "            targets[j] = data[row+delay][1]\n",
    "        samples = np.reshape(samples, (data.shape[-1], lookback//step, \n",
    "                                 len(rows)))\n",
    "        yield samples, targets\n",
    "    \n",
    "#every time you call the generator, will return the next group of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing the training, validation, and test generators\n",
    "#seqlen = lookback//step\n",
    "lookback = 36\n",
    "step = 6\n",
    "delay = 144  #from the past 10 days to predict the next day\n",
    "batch_size = 128\n",
    "\n",
    "#only training data need to randomly select batch\n",
    "train_gen = generator(float_data, \n",
    "                      lookback=lookback,\n",
    "                      delay=delay,\n",
    "                      min_index=0,\n",
    "                      max_index=200000,\n",
    "                      shuffle=True,\n",
    "                      step=step,\n",
    "                      batch_size=batch_size)\n",
    "\n",
    "val_gen = generator(float_data,\n",
    "                    lookback=lookback,\n",
    "                    delay=delay,\n",
    "                    min_index=200001,\n",
    "                    max_index=300000,\n",
    "                    step=step,\n",
    "                    batch_size=batch_size)\n",
    "\n",
    "test_gen = generator(float_data,\n",
    "                     lookback=lookback,\n",
    "                     delay=delay,\n",
    "                     min_index=300001,\n",
    "                     max_index=None,\n",
    "                     step=step,\n",
    "                     batch_size=batch_size)\n",
    "\n",
    "train_steps = (200001 - lookback) // batch_size\n",
    "val_steps = (300000 - 200001 - lookback) // batch_size\n",
    "test_steps = (len(float_data) - 300001 - lookback) // batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip(orig, norm):\n",
    "    ans = np.zeros_like(orig)\n",
    "    for i in range(orig.shape[1]):\n",
    "        if np.nansum(np.square(orig[:, i])) >= norm:\n",
    "            ans[:, i] = orig[:, i]/np.nansum(np.absolute(orig[:, i]))*norm\n",
    "        else:\n",
    "            ans[:, i] = orig[:, i]\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5, 0.5, 0.5],\n",
       "       [0.5, 0.5, 0.5]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip(5*np.ones((2,3)), 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: fit and feed forward are two seperate things\n",
    "# fit will use the trained parameters\n",
    "# pseudo inverse: matrix dimension will transpose\n",
    "# data need to be in columns\n",
    "class RNN(object):\n",
    "    def __init__(self, INPUT_DIM, HIDDEN_DIM, OUTPUT_DIM, N_BATCH, SEQ_LEN, NORM):\n",
    "        # N_BATCH: number of examples in one batch\n",
    "        # initialize weight matrix for input, hidden, output matrix\n",
    "        # these weights are shared\n",
    "        self.norm = NORM\n",
    "        self.hid = HIDDEN_DIM\n",
    "        self.nbatch = N_BATCH\n",
    "#         np.random.seed(0)\n",
    "        self.wi = np.random.normal(-0.1, 0.1, [HIDDEN_DIM, INPUT_DIM])\n",
    "#         np.random.seed(1)\n",
    "        self.wh = np.random.normal(-0.1, 0.1, [HIDDEN_DIM, HIDDEN_DIM])\n",
    "#         np.random.seed(2)\n",
    "        self.wo = np.random.normal(-0.1, 0.1, [OUTPUT_DIM, HIDDEN_DIM])\n",
    "\n",
    "        self.h = list()\n",
    "        self.u = list()\n",
    "        self.v = list()\n",
    "\n",
    "        # intialize variables in RNN\n",
    "        # NOTE: initial h is always initialized as zero matrix and is not trained!\n",
    "        for t in range(SEQ_LEN):\n",
    "            # could try initialize h to all zeros \n",
    "#             np.random.seed(t+10)\n",
    "            self.h.append(np.random.normal(-0.1, 0.1, [HIDDEN_DIM, N_BATCH]))\n",
    "#             np.random.seed(t+200)\n",
    "            self.u.append(np.random.normal(-0.1, 0.1, [HIDDEN_DIM, N_BATCH]))\n",
    "#             np.random.seed(t+400)\n",
    "            self.v.append(np.random.normal(-0.1, 0.1, [HIDDEN_DIM, N_BATCH]))\n",
    "        \n",
    "#         np.random.seed(5)\n",
    "        self.yT =  np.random.normal(-0.1, 0.1, [OUTPUT_DIM, N_BATCH])\n",
    "\n",
    "        self.wi_list = []\n",
    "        self.wh_list = []\n",
    "        self.wo_list = []\n",
    "\n",
    "        # same shape as the the final otuput\n",
    "        self.lambda_lagrange = np.ones((OUTPUT_DIM, N_BATCH))\n",
    "\n",
    "    def wi_update(self, ut, xt):\n",
    "        xt = clip(xt, self.norm)\n",
    "        pinv = np.linalg.pinv(xt)\n",
    "        wi = np.dot(ut, pinv)\n",
    "        return clip(wi, self.norm)\n",
    "\n",
    "    def wh_update(self, vt, hprev):\n",
    "        hprev = clip(hprev, self.norm)\n",
    "        pinv = np.linalg.pinv(hprev)\n",
    "        wh = np.dot(vt, pinv)\n",
    "        return clip(wh, self.norm) \n",
    "\n",
    "    def wo_update(self, yT, hT):\n",
    "        hT = clip(hT, self.norm)\n",
    "        pinv = np.linalg.pinv(hT)\n",
    "        wo = np.dot(yT, pinv)\n",
    "        return clip(wo, self.norm) \n",
    "\n",
    "    def ut_update(self, ut, vt, wi, xt, ht, alpha, gamma):\n",
    "        u_v = ut + vt \n",
    "        new_ut = np.zeros_like(ut)\n",
    "        sol1 = np.dot(wi, xt)  # u_v < 0\n",
    "        sol2 = (alpha*ht + gamma*np.dot(wi, xt) - alpha*vt)/(gamma+alpha)\n",
    "\n",
    "        new_ut[u_v>=0.] = sol2[u_v>=0.]\n",
    "        new_ut[u_v<0.] = sol1[u_v<0.]\n",
    "\n",
    "        return clip(new_ut, self.norm)\n",
    "\n",
    "    def vt_update(self, ut, vt, wh, hprev, ht, omega, alpha):\n",
    "        u_v = ut + vt \n",
    "        new_vt = np.zeros_like(vt)\n",
    "        sol1 = np.dot(wh, hprev) # u_v < 0 \n",
    "        sol2 = (omega*np.dot(wh, hprev) - alpha*ut + alpha*ht)/(omega+alpha)    \n",
    "        new_vt[u_v>=0.] = sol2[u_v>=0.]\n",
    "        new_vt[u_v<0] = sol1[u_v<0]\n",
    "\n",
    "        return clip(new_vt, self.norm)\n",
    "\n",
    "    def ht_update(self, omega, vnext, wh, alpha, ut, vt):\n",
    "        parta = omega*np.dot(wh, vnext) + alpha*relu(ut + vt)\n",
    "        partb = omega*np.dot(wh.T, wh) + alpha*np.eye(wh.shape[1])\n",
    "        partb = clip(partb, self.norm)\n",
    "        np.linalg.pinv(partb)\n",
    "        return clip(np.dot(np.linalg.pinv(partb), parta), self.norm)\n",
    "\n",
    "    # update last output\n",
    "    def hT_update(self, yT, wo):\n",
    "        wo = clip(wo, self.norm)\n",
    "        hT = np.dot(np.linalg.pinv(wo), yT)\n",
    "        return clip(hT, self.norm) \n",
    "\n",
    "    # necessary becoz yT is used in wo update\n",
    "    # target is in one-hot format\n",
    "    # target: (OUTPUT_DIM, N_BATCH)\n",
    "    def yT_update(self, target, beta, wo, hT, lambda_lagrange):\n",
    "        yT = (target + beta*np.dot(wo, hT) - lambda_lagrange/2)/(1+beta)\n",
    "        return clip(yT, self.norm) \n",
    "\n",
    "    def lambda_update(self, beta, yT, wo, hT):\n",
    "        lambda_up = beta*(yT - np.dot(wo, hT))\n",
    "        return clip(self.lambda_lagrange + lambda_up, self.norm) \n",
    "\n",
    "    # input shape: (N_BATCH, seq_len, INPUT_DIM)\n",
    "    # many-to-one\n",
    "    def feed_forward(self, inputs):\n",
    "        seq_len = inputs.shape[1]\n",
    "\n",
    "        hidden = np.zeros((self.hid, self.nbatch))\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            X = inputs[:, t, :]  #(INPUT_DIM, N_BATCH)\n",
    "            hidden = relu(np.dot(self.wi, X) + np.dot(self.wh, hidden))\n",
    "\n",
    "        output = np.dot(self.wo, hidden)\n",
    "        return output \n",
    "        # shape: (OUTPUT_DIM, N_BATCH)\n",
    "    \n",
    "    def fit(self, inputs, labels, alpha, beta, gamma, omega, val_gen):\n",
    "        # inputs: (INPUT_DIM, SEQ_LEN, N_BATCH)\n",
    "        seq_len = inputs.shape[1]\n",
    "        init_hidden = np.zeros((self.hid, self.nbatch))\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            xt = inputs[:, t, :]\n",
    "            self.wi = self.wi_update(self.u[t], xt)\n",
    "            self.u[t] = self.ut_update(self.u[t], self.v[t], self.wi, xt, self.h[t], alpha, gamma)\n",
    "            if t>0:\n",
    "                self.wh = self.wh_update(self.v[t], self.h[t-1])\n",
    "                self.v[t] = self.vt_update(self.u[t], self.v[t], self.wh, self.h[t-1], self.h[t], omega, alpha)\n",
    "            else:\n",
    "                # step 0\n",
    "                self.wh = self.wh_update(self.v[t], init_hidden)\n",
    "                self.v[t] = self.vt_update(self.u[t], self.v[t], self.wh, init_hidden, self.h[t], omega, alpha)\n",
    "            if t < seq_len-1:\n",
    "                self.h[t] = self.ht_update(omega, self.v[t+1], self.wh, alpha, self.u[t], self.v[t])\n",
    "            else:\n",
    "                self.h[t] = self.hT_update(self.yT, self.wo)\n",
    "        \n",
    "        self.wo = self.wo_update(self.yT, self.h[-1])\n",
    "        self.yT = self.yT_update(labels, beta, self.wo, self.h[-1], self.lambda_lagrange)\n",
    "        self.lambda_lagrange = self.lambda_update(beta, self.yT, self.wo, self.h[-1])\n",
    "\n",
    "#         ## add accuracy to evaluate function if needed\n",
    "#         for (data, labels) in val_gen:\n",
    "#             loss = self.evaluate(data, labels)\n",
    "#             break\n",
    "#         return loss \n",
    "\n",
    "    def warming(self, inputs, labels, alpha, beta, gamma, omega, epochs):\n",
    "        # inputs: (INPUT_DIM, SEQ_LEN, N_BATCH)\n",
    "        seq_len = inputs.shape[1]\n",
    "        init_hidden = np.zeros((self.hid, self.nbatch))\n",
    "        for ep in range(epochs):\n",
    "#             print (\"------ Warming: {:d} ------\".format(ep))\n",
    "            for t in range(seq_len):\n",
    "                xt = inputs[:, t, :]\n",
    "                self.wi = self.wi_update(self.u[t], xt)\n",
    "#                 print (\"wi: \", self.wi[0][0])\n",
    "                self.u[t] = self.ut_update(self.u[t], self.v[t], self.wi, xt, self.h[t], alpha, gamma)\n",
    "#                 print (\"ut: \", self.u[t][0][0])\n",
    "                if t>0:\n",
    "                    self.wh = self.wh_update(self.v[t], self.h[t-1])\n",
    "                    self.v[t] = self.vt_update(self.u[t], self.v[t], self.wh, self.h[t-1], self.h[t], omega, alpha)\n",
    "                else:\n",
    "                    # step 0\n",
    "                    self.wh = self.wh_update(self.v[t], init_hidden)\n",
    "                    self.v[t] = self.vt_update(self.u[t], self.v[t], self.wh, init_hidden, self.h[t], omega, alpha)\n",
    "#                 print (\"wh: \", self.wh[0][0])\n",
    "#                 print (\"vt: \", self.v[t][0][0])\n",
    "                if t < seq_len-1:\n",
    "                    self.h[t] = self.ht_update(omega, self.v[t+1], self.wh, alpha, self.u[t], self.v[t])\n",
    "                else:\n",
    "                    self.h[t] = self.hT_update(self.yT, self.wo)\n",
    "#                 print (\"ht: \", self.h[t][0][0])\n",
    "                \n",
    "#                 self.wi_list.append(self.wi[2][2])\n",
    "#                 self.wh_list.append(self.wh[2][2])\n",
    "#                 self.wo_list.append(self.wo[0][1])\n",
    "                \n",
    "#             plt.plot(range(len(self.wi_list)), self.wi_list)\n",
    "#             plt.plot(range(len(self.wi_list)), self.wh_list)\n",
    "#             plt.plot(range(len(self.wi_list)), self.wo_list)\n",
    "#             plt.legend(['wi', 'wh', 'wo'], loc='upper left')\n",
    "\n",
    "#             plt.show()\n",
    "            \n",
    "            self.wo = self.wo_update(self.yT, self.h[-1])\n",
    "            self.yT = self.yT_update(labels, beta, self.wo, self.h[-1], self.lambda_lagrange)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    def evaluate(self, inputs, labels):\n",
    "        # inputs: (input_dim, seq_len, N_BATCH)\n",
    "        # labels: (output_dim, N_BATCH)\n",
    "        preds = self.feed_forward(inputs)\n",
    "        # (OUTPUT_DIM, N_BATCH)\n",
    "        labels = np.asarray(labels)\n",
    "\n",
    "        loss = np.mean(np.square(np.subtract(preds, labels)))\n",
    "        return loss \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 14\n",
    "n_outputs = 1\n",
    "n_hiddens = 32\n",
    "n_batch = 128\n",
    "seqlen = lookback // step\n",
    "train_epochs = 5\n",
    "warm_epochs = 5\n",
    "\n",
    "alpha = 50\n",
    "beta = 0.1 \n",
    "gamma = 0.1 \n",
    "omega = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed:  0.1 0.1 0.1 0.1\n",
      "failed:  0.1 0.1 0.1 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sichenglei/Desktop/kaggle/salt/venv/lib/python3.6/site-packages/ipykernel_launcher.py:86: RuntimeWarning: overflow encountered in multiply\n",
      "/Users/sichenglei/Desktop/kaggle/salt/venv/lib/python3.6/site-packages/numpy/core/fromnumeric.py:83: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/Users/sichenglei/Desktop/kaggle/salt/venv/lib/python3.6/site-packages/numpy/linalg/linalg.py:1864: RuntimeWarning: invalid value encountered in greater\n",
      "  large = s > cutoff\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed:  0.1 0.1 0.1 10.0\n",
      "failed:  0.1 0.1 0.1 50.0\n",
      "failed:  0.1 0.1 0.1 100.0\n",
      "failed:  0.1 0.1 1.0 0.1\n",
      "failed:  0.1 0.1 1.0 1.0\n",
      "failed:  0.1 0.1 1.0 10.0\n",
      "failed:  0.1 0.1 1.0 50.0\n",
      "failed:  0.1 0.1 1.0 100.0\n",
      "failed:  0.1 0.1 10.0 0.1\n",
      "failed:  0.1 0.1 10.0 1.0\n",
      "failed:  0.1 0.1 10.0 10.0\n",
      "failed:  0.1 0.1 10.0 50.0\n",
      "failed:  0.1 0.1 10.0 100.0\n",
      "failed:  0.1 0.1 50.0 0.1\n",
      "failed:  0.1 0.1 50.0 1.0\n",
      "failed:  0.1 0.1 50.0 10.0\n",
      "failed:  0.1 0.1 50.0 50.0\n",
      "failed:  0.1 0.1 50.0 100.0\n",
      "failed:  0.1 0.1 100.0 0.1\n",
      "failed:  0.1 0.1 100.0 1.0\n",
      "failed:  0.1 0.1 100.0 10.0\n",
      "failed:  0.1 0.1 100.0 50.0\n",
      "failed:  0.1 0.1 100.0 100.0\n",
      "failed:  0.1 1.0 0.1 0.1\n",
      "failed:  0.1 1.0 0.1 1.0\n",
      "failed:  0.1 1.0 0.1 10.0\n",
      "failed:  0.1 1.0 0.1 50.0\n",
      "failed:  0.1 1.0 0.1 100.0\n",
      "failed:  0.1 1.0 1.0 0.1\n",
      "failed:  0.1 1.0 1.0 1.0\n",
      "failed:  0.1 1.0 1.0 10.0\n",
      "failed:  0.1 1.0 1.0 50.0\n",
      "failed:  0.1 1.0 1.0 100.0\n",
      "failed:  0.1 1.0 10.0 0.1\n",
      "failed:  0.1 1.0 10.0 1.0\n",
      "failed:  0.1 1.0 10.0 10.0\n",
      "failed:  0.1 1.0 10.0 50.0\n",
      "failed:  0.1 1.0 10.0 100.0\n",
      "failed:  0.1 1.0 50.0 0.1\n",
      "failed:  0.1 1.0 50.0 1.0\n",
      "failed:  0.1 1.0 50.0 10.0\n",
      "failed:  0.1 1.0 50.0 50.0\n",
      "failed:  0.1 1.0 50.0 100.0\n",
      "failed:  0.1 1.0 100.0 0.1\n",
      "failed:  0.1 1.0 100.0 1.0\n",
      "failed:  0.1 1.0 100.0 10.0\n",
      "failed:  0.1 1.0 100.0 50.0\n",
      "failed:  0.1 1.0 100.0 100.0\n",
      "failed:  0.1 10.0 0.1 0.1\n",
      "failed:  0.1 10.0 0.1 1.0\n",
      "failed:  0.1 10.0 0.1 10.0\n",
      "failed:  0.1 10.0 0.1 50.0\n",
      "failed:  0.1 10.0 0.1 100.0\n",
      "failed:  0.1 10.0 1.0 0.1\n",
      "failed:  0.1 10.0 1.0 1.0\n",
      "failed:  0.1 10.0 1.0 10.0\n",
      "failed:  0.1 10.0 1.0 50.0\n",
      "failed:  0.1 10.0 1.0 100.0\n",
      "failed:  0.1 10.0 10.0 0.1\n",
      "failed:  0.1 10.0 10.0 1.0\n",
      "failed:  0.1 10.0 10.0 10.0\n",
      "failed:  0.1 10.0 10.0 50.0\n",
      "failed:  0.1 10.0 10.0 100.0\n",
      "failed:  0.1 10.0 50.0 0.1\n",
      "failed:  0.1 10.0 50.0 1.0\n",
      "failed:  0.1 10.0 50.0 10.0\n",
      "failed:  0.1 10.0 50.0 50.0\n",
      "failed:  0.1 10.0 50.0 100.0\n",
      "failed:  0.1 10.0 100.0 0.1\n",
      "failed:  0.1 10.0 100.0 1.0\n",
      "failed:  0.1 10.0 100.0 10.0\n",
      "failed:  0.1 10.0 100.0 50.0\n",
      "failed:  0.1 10.0 100.0 100.0\n",
      "failed:  0.1 50.0 0.1 0.1\n",
      "failed:  0.1 50.0 0.1 1.0\n",
      "failed:  0.1 50.0 0.1 10.0\n",
      "failed:  0.1 50.0 0.1 50.0\n",
      "failed:  0.1 50.0 0.1 100.0\n",
      "failed:  0.1 50.0 1.0 0.1\n",
      "failed:  0.1 50.0 1.0 1.0\n",
      "failed:  0.1 50.0 1.0 10.0\n",
      "failed:  0.1 50.0 1.0 50.0\n",
      "failed:  0.1 50.0 1.0 100.0\n",
      "failed:  0.1 50.0 10.0 0.1\n",
      "failed:  0.1 50.0 10.0 1.0\n",
      "failed:  0.1 50.0 10.0 10.0\n",
      "failed:  0.1 50.0 10.0 50.0\n",
      "failed:  0.1 50.0 10.0 100.0\n",
      "failed:  0.1 50.0 50.0 0.1\n",
      "failed:  0.1 50.0 50.0 1.0\n",
      "failed:  0.1 50.0 50.0 10.0\n",
      "failed:  0.1 50.0 50.0 50.0\n",
      "failed:  0.1 50.0 50.0 100.0\n",
      "failed:  0.1 50.0 100.0 0.1\n",
      "failed:  0.1 50.0 100.0 1.0\n",
      "failed:  0.1 50.0 100.0 10.0\n",
      "failed:  0.1 50.0 100.0 50.0\n",
      "failed:  0.1 50.0 100.0 100.0\n",
      "failed:  0.1 100.0 0.1 0.1\n",
      "failed:  0.1 100.0 0.1 1.0\n",
      "failed:  0.1 100.0 0.1 10.0\n",
      "failed:  0.1 100.0 0.1 50.0\n",
      "failed:  0.1 100.0 0.1 100.0\n",
      "failed:  0.1 100.0 1.0 0.1\n",
      "failed:  0.1 100.0 1.0 1.0\n",
      "failed:  0.1 100.0 1.0 10.0\n",
      "failed:  0.1 100.0 1.0 50.0\n",
      "failed:  0.1 100.0 1.0 100.0\n",
      "failed:  0.1 100.0 10.0 0.1\n",
      "failed:  0.1 100.0 10.0 1.0\n",
      "failed:  0.1 100.0 10.0 10.0\n",
      "failed:  0.1 100.0 10.0 50.0\n",
      "failed:  0.1 100.0 10.0 100.0\n",
      "failed:  0.1 100.0 50.0 0.1\n",
      "failed:  0.1 100.0 50.0 1.0\n",
      "failed:  0.1 100.0 50.0 10.0\n",
      "failed:  0.1 100.0 50.0 50.0\n",
      "failed:  0.1 100.0 50.0 100.0\n",
      "failed:  0.1 100.0 100.0 0.1\n",
      "failed:  0.1 100.0 100.0 1.0\n",
      "failed:  0.1 100.0 100.0 10.0\n",
      "failed:  0.1 100.0 100.0 50.0\n",
      "failed:  0.1 100.0 100.0 100.0\n",
      "failed:  1.0 0.1 0.1 0.1\n",
      "failed:  1.0 0.1 0.1 1.0\n",
      "failed:  1.0 0.1 0.1 10.0\n",
      "failed:  1.0 0.1 0.1 50.0\n",
      "failed:  1.0 0.1 0.1 100.0\n",
      "failed:  1.0 0.1 1.0 0.1\n",
      "failed:  1.0 0.1 1.0 1.0\n",
      "failed:  1.0 0.1 1.0 10.0\n",
      "failed:  1.0 0.1 1.0 50.0\n",
      "failed:  1.0 0.1 1.0 100.0\n",
      "failed:  1.0 0.1 10.0 0.1\n",
      "failed:  1.0 0.1 10.0 1.0\n",
      "failed:  1.0 0.1 10.0 10.0\n",
      "failed:  1.0 0.1 10.0 50.0\n",
      "failed:  1.0 0.1 10.0 100.0\n",
      "failed:  1.0 0.1 50.0 0.1\n",
      "failed:  1.0 0.1 50.0 1.0\n",
      "failed:  1.0 0.1 50.0 10.0\n",
      "failed:  1.0 0.1 50.0 50.0\n",
      "failed:  1.0 0.1 50.0 100.0\n",
      "failed:  1.0 0.1 100.0 0.1\n",
      "failed:  1.0 0.1 100.0 1.0\n",
      "failed:  1.0 0.1 100.0 10.0\n",
      "failed:  1.0 0.1 100.0 50.0\n",
      "failed:  1.0 0.1 100.0 100.0\n",
      "failed:  1.0 1.0 0.1 0.1\n",
      "failed:  1.0 1.0 0.1 1.0\n",
      "failed:  1.0 1.0 0.1 10.0\n",
      "failed:  1.0 1.0 0.1 50.0\n",
      "failed:  1.0 1.0 0.1 100.0\n",
      "failed:  1.0 1.0 1.0 0.1\n",
      "failed:  1.0 1.0 1.0 1.0\n",
      "failed:  1.0 1.0 1.0 10.0\n",
      "failed:  1.0 1.0 1.0 50.0\n",
      "failed:  1.0 1.0 1.0 100.0\n",
      "failed:  1.0 1.0 10.0 0.1\n",
      "failed:  1.0 1.0 10.0 1.0\n",
      "failed:  1.0 1.0 10.0 10.0\n",
      "failed:  1.0 1.0 10.0 50.0\n",
      "failed:  1.0 1.0 10.0 100.0\n",
      "failed:  1.0 1.0 50.0 0.1\n",
      "failed:  1.0 1.0 50.0 1.0\n",
      "failed:  1.0 1.0 50.0 10.0\n",
      "failed:  1.0 1.0 50.0 50.0\n",
      "failed:  1.0 1.0 50.0 100.0\n",
      "failed:  1.0 1.0 100.0 0.1\n",
      "failed:  1.0 1.0 100.0 1.0\n",
      "failed:  1.0 1.0 100.0 10.0\n",
      "failed:  1.0 1.0 100.0 50.0\n",
      "failed:  1.0 1.0 100.0 100.0\n",
      "failed:  1.0 10.0 0.1 0.1\n",
      "failed:  1.0 10.0 0.1 1.0\n",
      "failed:  1.0 10.0 0.1 10.0\n",
      "failed:  1.0 10.0 0.1 50.0\n",
      "failed:  1.0 10.0 0.1 100.0\n",
      "failed:  1.0 10.0 1.0 0.1\n",
      "failed:  1.0 10.0 1.0 1.0\n",
      "failed:  1.0 10.0 1.0 10.0\n",
      "failed:  1.0 10.0 1.0 50.0\n",
      "failed:  1.0 10.0 1.0 100.0\n",
      "failed:  1.0 10.0 10.0 0.1\n",
      "failed:  1.0 10.0 10.0 1.0\n",
      "failed:  1.0 10.0 10.0 10.0\n",
      "failed:  1.0 10.0 10.0 50.0\n",
      "failed:  1.0 10.0 10.0 100.0\n",
      "failed:  1.0 10.0 50.0 0.1\n",
      "failed:  1.0 10.0 50.0 1.0\n",
      "failed:  1.0 10.0 50.0 10.0\n",
      "failed:  1.0 10.0 50.0 50.0\n",
      "failed:  1.0 10.0 50.0 100.0\n",
      "failed:  1.0 10.0 100.0 0.1\n",
      "failed:  1.0 10.0 100.0 1.0\n",
      "failed:  1.0 10.0 100.0 10.0\n",
      "failed:  1.0 10.0 100.0 50.0\n",
      "failed:  1.0 10.0 100.0 100.0\n",
      "failed:  1.0 50.0 0.1 0.1\n",
      "failed:  1.0 50.0 0.1 1.0\n",
      "failed:  1.0 50.0 0.1 10.0\n",
      "failed:  1.0 50.0 0.1 50.0\n",
      "failed:  1.0 50.0 0.1 100.0\n",
      "failed:  1.0 50.0 1.0 0.1\n",
      "failed:  1.0 50.0 1.0 1.0\n",
      "failed:  1.0 50.0 1.0 10.0\n",
      "failed:  1.0 50.0 1.0 50.0\n",
      "failed:  1.0 50.0 1.0 100.0\n",
      "failed:  1.0 50.0 10.0 0.1\n",
      "failed:  1.0 50.0 10.0 1.0\n",
      "failed:  1.0 50.0 10.0 10.0\n",
      "failed:  1.0 50.0 10.0 50.0\n",
      "failed:  1.0 50.0 10.0 100.0\n",
      "failed:  1.0 50.0 50.0 0.1\n",
      "failed:  1.0 50.0 50.0 1.0\n",
      "failed:  1.0 50.0 50.0 10.0\n",
      "failed:  1.0 50.0 50.0 50.0\n",
      "failed:  1.0 50.0 50.0 100.0\n",
      "failed:  1.0 50.0 100.0 0.1\n",
      "failed:  1.0 50.0 100.0 1.0\n",
      "failed:  1.0 50.0 100.0 10.0\n",
      "failed:  1.0 50.0 100.0 50.0\n",
      "failed:  1.0 50.0 100.0 100.0\n",
      "failed:  1.0 100.0 0.1 0.1\n",
      "failed:  1.0 100.0 0.1 1.0\n",
      "failed:  1.0 100.0 0.1 10.0\n",
      "failed:  1.0 100.0 0.1 50.0\n",
      "failed:  1.0 100.0 0.1 100.0\n",
      "failed:  1.0 100.0 1.0 0.1\n",
      "failed:  1.0 100.0 1.0 1.0\n",
      "failed:  1.0 100.0 1.0 10.0\n",
      "failed:  1.0 100.0 1.0 50.0\n",
      "failed:  1.0 100.0 1.0 100.0\n",
      "failed:  1.0 100.0 10.0 0.1\n",
      "failed:  1.0 100.0 10.0 1.0\n",
      "failed:  1.0 100.0 10.0 10.0\n",
      "failed:  1.0 100.0 10.0 50.0\n",
      "failed:  1.0 100.0 10.0 100.0\n",
      "failed:  1.0 100.0 50.0 0.1\n",
      "failed:  1.0 100.0 50.0 1.0\n",
      "failed:  1.0 100.0 50.0 10.0\n",
      "failed:  1.0 100.0 50.0 50.0\n",
      "failed:  1.0 100.0 50.0 100.0\n",
      "failed:  1.0 100.0 100.0 0.1\n",
      "failed:  1.0 100.0 100.0 1.0\n",
      "failed:  1.0 100.0 100.0 10.0\n",
      "failed:  1.0 100.0 100.0 50.0\n",
      "failed:  1.0 100.0 100.0 100.0\n",
      "failed:  10.0 0.1 0.1 0.1\n",
      "failed:  10.0 0.1 0.1 1.0\n",
      "failed:  10.0 0.1 0.1 10.0\n",
      "failed:  10.0 0.1 0.1 50.0\n",
      "failed:  10.0 0.1 0.1 100.0\n",
      "failed:  10.0 0.1 1.0 0.1\n",
      "failed:  10.0 0.1 1.0 1.0\n",
      "failed:  10.0 0.1 1.0 10.0\n",
      "failed:  10.0 0.1 1.0 50.0\n",
      "failed:  10.0 0.1 1.0 100.0\n",
      "failed:  10.0 0.1 10.0 0.1\n",
      "failed:  10.0 0.1 10.0 1.0\n",
      "failed:  10.0 0.1 10.0 10.0\n",
      "failed:  10.0 0.1 10.0 50.0\n",
      "failed:  10.0 0.1 10.0 100.0\n",
      "failed:  10.0 0.1 50.0 0.1\n",
      "failed:  10.0 0.1 50.0 1.0\n",
      "failed:  10.0 0.1 50.0 10.0\n",
      "failed:  10.0 0.1 50.0 50.0\n",
      "failed:  10.0 0.1 50.0 100.0\n",
      "failed:  10.0 0.1 100.0 0.1\n",
      "failed:  10.0 0.1 100.0 1.0\n",
      "failed:  10.0 0.1 100.0 10.0\n",
      "failed:  10.0 0.1 100.0 50.0\n",
      "failed:  10.0 0.1 100.0 100.0\n",
      "failed:  10.0 1.0 0.1 0.1\n",
      "failed:  10.0 1.0 0.1 1.0\n",
      "failed:  10.0 1.0 0.1 10.0\n",
      "failed:  10.0 1.0 0.1 50.0\n",
      "failed:  10.0 1.0 0.1 100.0\n",
      "failed:  10.0 1.0 1.0 0.1\n",
      "failed:  10.0 1.0 1.0 1.0\n",
      "failed:  10.0 1.0 1.0 10.0\n",
      "failed:  10.0 1.0 1.0 50.0\n",
      "failed:  10.0 1.0 1.0 100.0\n",
      "failed:  10.0 1.0 10.0 0.1\n",
      "failed:  10.0 1.0 10.0 1.0\n",
      "failed:  10.0 1.0 10.0 10.0\n",
      "failed:  10.0 1.0 10.0 50.0\n",
      "failed:  10.0 1.0 10.0 100.0\n",
      "failed:  10.0 1.0 50.0 0.1\n",
      "failed:  10.0 1.0 50.0 1.0\n",
      "failed:  10.0 1.0 50.0 10.0\n",
      "failed:  10.0 1.0 50.0 50.0\n",
      "failed:  10.0 1.0 50.0 100.0\n",
      "failed:  10.0 1.0 100.0 0.1\n",
      "failed:  10.0 1.0 100.0 1.0\n",
      "failed:  10.0 1.0 100.0 10.0\n",
      "failed:  10.0 1.0 100.0 50.0\n",
      "failed:  10.0 1.0 100.0 100.0\n",
      "failed:  10.0 10.0 0.1 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed:  10.0 10.0 0.1 1.0\n",
      "failed:  10.0 10.0 0.1 10.0\n",
      "failed:  10.0 10.0 0.1 50.0\n",
      "failed:  10.0 10.0 0.1 100.0\n",
      "failed:  10.0 10.0 1.0 0.1\n",
      "failed:  10.0 10.0 1.0 1.0\n",
      "failed:  10.0 10.0 1.0 10.0\n",
      "failed:  10.0 10.0 1.0 50.0\n",
      "failed:  10.0 10.0 1.0 100.0\n",
      "failed:  10.0 10.0 10.0 0.1\n",
      "failed:  10.0 10.0 10.0 1.0\n",
      "failed:  10.0 10.0 10.0 10.0\n",
      "failed:  10.0 10.0 10.0 50.0\n",
      "failed:  10.0 10.0 10.0 100.0\n",
      "failed:  10.0 10.0 50.0 0.1\n",
      "failed:  10.0 10.0 50.0 1.0\n",
      "failed:  10.0 10.0 50.0 10.0\n",
      "failed:  10.0 10.0 50.0 50.0\n",
      "failed:  10.0 10.0 50.0 100.0\n",
      "failed:  10.0 10.0 100.0 0.1\n",
      "failed:  10.0 10.0 100.0 1.0\n",
      "failed:  10.0 10.0 100.0 10.0\n",
      "failed:  10.0 10.0 100.0 50.0\n",
      "failed:  10.0 10.0 100.0 100.0\n",
      "failed:  10.0 50.0 0.1 0.1\n",
      "failed:  10.0 50.0 0.1 1.0\n",
      "failed:  10.0 50.0 0.1 10.0\n",
      "failed:  10.0 50.0 0.1 50.0\n",
      "failed:  10.0 50.0 0.1 100.0\n",
      "failed:  10.0 50.0 1.0 0.1\n",
      "failed:  10.0 50.0 1.0 1.0\n",
      "failed:  10.0 50.0 1.0 10.0\n",
      "failed:  10.0 50.0 1.0 50.0\n",
      "failed:  10.0 50.0 1.0 100.0\n",
      "failed:  10.0 50.0 10.0 0.1\n",
      "failed:  10.0 50.0 10.0 1.0\n",
      "failed:  10.0 50.0 10.0 10.0\n",
      "failed:  10.0 50.0 10.0 50.0\n",
      "failed:  10.0 50.0 10.0 100.0\n",
      "failed:  10.0 50.0 50.0 0.1\n",
      "failed:  10.0 50.0 50.0 1.0\n",
      "failed:  10.0 50.0 50.0 10.0\n",
      "failed:  10.0 50.0 50.0 50.0\n",
      "failed:  10.0 50.0 50.0 100.0\n",
      "failed:  10.0 50.0 100.0 0.1\n",
      "failed:  10.0 50.0 100.0 1.0\n",
      "failed:  10.0 50.0 100.0 10.0\n",
      "failed:  10.0 50.0 100.0 50.0\n",
      "failed:  10.0 50.0 100.0 100.0\n",
      "failed:  10.0 100.0 0.1 0.1\n",
      "failed:  10.0 100.0 0.1 1.0\n",
      "failed:  10.0 100.0 0.1 10.0\n",
      "failed:  10.0 100.0 0.1 50.0\n",
      "failed:  10.0 100.0 0.1 100.0\n",
      "failed:  10.0 100.0 1.0 0.1\n",
      "failed:  10.0 100.0 1.0 1.0\n",
      "failed:  10.0 100.0 1.0 10.0\n",
      "failed:  10.0 100.0 1.0 50.0\n",
      "failed:  10.0 100.0 1.0 100.0\n",
      "failed:  10.0 100.0 10.0 0.1\n",
      "failed:  10.0 100.0 10.0 1.0\n",
      "failed:  10.0 100.0 10.0 10.0\n",
      "failed:  10.0 100.0 10.0 50.0\n",
      "failed:  10.0 100.0 10.0 100.0\n",
      "failed:  10.0 100.0 50.0 0.1\n",
      "failed:  10.0 100.0 50.0 1.0\n",
      "failed:  10.0 100.0 50.0 10.0\n",
      "failed:  10.0 100.0 50.0 50.0\n",
      "failed:  10.0 100.0 50.0 100.0\n",
      "failed:  10.0 100.0 100.0 0.1\n",
      "failed:  10.0 100.0 100.0 1.0\n",
      "failed:  10.0 100.0 100.0 10.0\n",
      "failed:  10.0 100.0 100.0 50.0\n",
      "failed:  10.0 100.0 100.0 100.0\n",
      "failed:  50.0 0.1 0.1 0.1\n",
      "failed:  50.0 0.1 0.1 1.0\n",
      "failed:  50.0 0.1 0.1 10.0\n",
      "failed:  50.0 0.1 0.1 50.0\n",
      "failed:  50.0 0.1 0.1 100.0\n",
      "failed:  50.0 0.1 1.0 0.1\n",
      "failed:  50.0 0.1 1.0 1.0\n",
      "failed:  50.0 0.1 1.0 10.0\n",
      "failed:  50.0 0.1 1.0 50.0\n",
      "failed:  50.0 0.1 1.0 100.0\n",
      "failed:  50.0 0.1 10.0 0.1\n",
      "failed:  50.0 0.1 10.0 1.0\n",
      "failed:  50.0 0.1 10.0 10.0\n",
      "failed:  50.0 0.1 10.0 50.0\n",
      "failed:  50.0 0.1 10.0 100.0\n",
      "failed:  50.0 0.1 50.0 0.1\n",
      "failed:  50.0 0.1 50.0 1.0\n",
      "failed:  50.0 0.1 50.0 10.0\n",
      "failed:  50.0 0.1 50.0 50.0\n",
      "failed:  50.0 0.1 50.0 100.0\n",
      "failed:  50.0 0.1 100.0 0.1\n",
      "failed:  50.0 0.1 100.0 1.0\n",
      "failed:  50.0 0.1 100.0 10.0\n",
      "failed:  50.0 0.1 100.0 50.0\n",
      "failed:  50.0 0.1 100.0 100.0\n",
      "failed:  50.0 1.0 0.1 0.1\n",
      "failed:  50.0 1.0 0.1 1.0\n",
      "failed:  50.0 1.0 0.1 10.0\n",
      "failed:  50.0 1.0 0.1 50.0\n",
      "failed:  50.0 1.0 0.1 100.0\n",
      "failed:  50.0 1.0 1.0 0.1\n",
      "failed:  50.0 1.0 1.0 1.0\n",
      "failed:  50.0 1.0 1.0 10.0\n",
      "failed:  50.0 1.0 1.0 50.0\n",
      "failed:  50.0 1.0 1.0 100.0\n",
      "failed:  50.0 1.0 10.0 0.1\n",
      "failed:  50.0 1.0 10.0 1.0\n",
      "failed:  50.0 1.0 10.0 10.0\n",
      "failed:  50.0 1.0 10.0 50.0\n",
      "failed:  50.0 1.0 10.0 100.0\n",
      "failed:  50.0 1.0 50.0 0.1\n",
      "failed:  50.0 1.0 50.0 1.0\n",
      "failed:  50.0 1.0 50.0 10.0\n",
      "failed:  50.0 1.0 50.0 50.0\n",
      "failed:  50.0 1.0 50.0 100.0\n",
      "failed:  50.0 1.0 100.0 0.1\n",
      "failed:  50.0 1.0 100.0 1.0\n",
      "failed:  50.0 1.0 100.0 10.0\n",
      "failed:  50.0 1.0 100.0 50.0\n",
      "failed:  50.0 1.0 100.0 100.0\n",
      "failed:  50.0 10.0 0.1 0.1\n",
      "failed:  50.0 10.0 0.1 1.0\n",
      "failed:  50.0 10.0 0.1 10.0\n",
      "failed:  50.0 10.0 0.1 50.0\n",
      "failed:  50.0 10.0 0.1 100.0\n",
      "failed:  50.0 10.0 1.0 0.1\n",
      "failed:  50.0 10.0 1.0 1.0\n",
      "failed:  50.0 10.0 1.0 10.0\n",
      "failed:  50.0 10.0 1.0 50.0\n",
      "failed:  50.0 10.0 1.0 100.0\n",
      "failed:  50.0 10.0 10.0 0.1\n",
      "failed:  50.0 10.0 10.0 1.0\n",
      "failed:  50.0 10.0 10.0 10.0\n",
      "failed:  50.0 10.0 10.0 50.0\n",
      "failed:  50.0 10.0 10.0 100.0\n",
      "failed:  50.0 10.0 50.0 0.1\n",
      "failed:  50.0 10.0 50.0 1.0\n",
      "failed:  50.0 10.0 50.0 10.0\n",
      "failed:  50.0 10.0 50.0 50.0\n",
      "failed:  50.0 10.0 50.0 100.0\n",
      "failed:  50.0 10.0 100.0 0.1\n",
      "failed:  50.0 10.0 100.0 1.0\n",
      "failed:  50.0 10.0 100.0 10.0\n",
      "failed:  50.0 10.0 100.0 50.0\n",
      "failed:  50.0 10.0 100.0 100.0\n",
      "failed:  50.0 50.0 0.1 0.1\n",
      "failed:  50.0 50.0 0.1 1.0\n",
      "failed:  50.0 50.0 0.1 10.0\n",
      "failed:  50.0 50.0 0.1 50.0\n",
      "failed:  50.0 50.0 0.1 100.0\n",
      "failed:  50.0 50.0 1.0 0.1\n",
      "failed:  50.0 50.0 1.0 1.0\n",
      "failed:  50.0 50.0 1.0 10.0\n",
      "failed:  50.0 50.0 1.0 50.0\n",
      "failed:  50.0 50.0 1.0 100.0\n",
      "failed:  50.0 50.0 10.0 0.1\n",
      "failed:  50.0 50.0 10.0 1.0\n",
      "failed:  50.0 50.0 10.0 10.0\n",
      "failed:  50.0 50.0 10.0 50.0\n",
      "failed:  50.0 50.0 10.0 100.0\n",
      "failed:  50.0 50.0 50.0 0.1\n",
      "failed:  50.0 50.0 50.0 1.0\n",
      "failed:  50.0 50.0 50.0 10.0\n",
      "failed:  50.0 50.0 50.0 50.0\n",
      "failed:  50.0 50.0 50.0 100.0\n",
      "failed:  50.0 50.0 100.0 0.1\n",
      "failed:  50.0 50.0 100.0 1.0\n",
      "failed:  50.0 50.0 100.0 10.0\n",
      "failed:  50.0 50.0 100.0 50.0\n",
      "failed:  50.0 50.0 100.0 100.0\n",
      "failed:  50.0 100.0 0.1 0.1\n",
      "failed:  50.0 100.0 0.1 1.0\n",
      "failed:  50.0 100.0 0.1 10.0\n",
      "failed:  50.0 100.0 0.1 50.0\n",
      "failed:  50.0 100.0 0.1 100.0\n",
      "failed:  50.0 100.0 1.0 0.1\n",
      "failed:  50.0 100.0 1.0 1.0\n",
      "failed:  50.0 100.0 1.0 10.0\n",
      "failed:  50.0 100.0 1.0 50.0\n",
      "failed:  50.0 100.0 1.0 100.0\n",
      "failed:  50.0 100.0 10.0 0.1\n",
      "failed:  50.0 100.0 10.0 1.0\n",
      "failed:  50.0 100.0 10.0 10.0\n",
      "failed:  50.0 100.0 10.0 50.0\n",
      "failed:  50.0 100.0 10.0 100.0\n",
      "failed:  50.0 100.0 50.0 0.1\n",
      "failed:  50.0 100.0 50.0 1.0\n",
      "failed:  50.0 100.0 50.0 10.0\n",
      "failed:  50.0 100.0 50.0 50.0\n",
      "failed:  50.0 100.0 50.0 100.0\n",
      "failed:  50.0 100.0 100.0 0.1\n",
      "failed:  50.0 100.0 100.0 1.0\n",
      "failed:  50.0 100.0 100.0 10.0\n",
      "failed:  50.0 100.0 100.0 50.0\n",
      "failed:  50.0 100.0 100.0 100.0\n",
      "failed:  100.0 0.1 0.1 0.1\n",
      "failed:  100.0 0.1 0.1 1.0\n",
      "failed:  100.0 0.1 0.1 10.0\n",
      "failed:  100.0 0.1 0.1 50.0\n",
      "failed:  100.0 0.1 0.1 100.0\n",
      "failed:  100.0 0.1 1.0 0.1\n",
      "failed:  100.0 0.1 1.0 1.0\n",
      "failed:  100.0 0.1 1.0 10.0\n",
      "failed:  100.0 0.1 1.0 50.0\n",
      "failed:  100.0 0.1 1.0 100.0\n",
      "failed:  100.0 0.1 10.0 0.1\n",
      "failed:  100.0 0.1 10.0 1.0\n",
      "failed:  100.0 0.1 10.0 10.0\n",
      "failed:  100.0 0.1 10.0 50.0\n",
      "failed:  100.0 0.1 10.0 100.0\n",
      "failed:  100.0 0.1 50.0 0.1\n",
      "failed:  100.0 0.1 50.0 1.0\n",
      "failed:  100.0 0.1 50.0 10.0\n",
      "failed:  100.0 0.1 50.0 50.0\n",
      "failed:  100.0 0.1 50.0 100.0\n",
      "failed:  100.0 0.1 100.0 0.1\n",
      "failed:  100.0 0.1 100.0 1.0\n",
      "failed:  100.0 0.1 100.0 10.0\n",
      "failed:  100.0 0.1 100.0 50.0\n",
      "failed:  100.0 0.1 100.0 100.0\n",
      "failed:  100.0 1.0 0.1 0.1\n",
      "failed:  100.0 1.0 0.1 1.0\n",
      "failed:  100.0 1.0 0.1 10.0\n",
      "failed:  100.0 1.0 0.1 50.0\n",
      "failed:  100.0 1.0 0.1 100.0\n",
      "failed:  100.0 1.0 1.0 0.1\n",
      "failed:  100.0 1.0 1.0 1.0\n",
      "failed:  100.0 1.0 1.0 10.0\n",
      "failed:  100.0 1.0 1.0 50.0\n",
      "failed:  100.0 1.0 1.0 100.0\n",
      "failed:  100.0 1.0 10.0 0.1\n",
      "failed:  100.0 1.0 10.0 1.0\n",
      "failed:  100.0 1.0 10.0 10.0\n",
      "failed:  100.0 1.0 10.0 50.0\n",
      "failed:  100.0 1.0 10.0 100.0\n",
      "failed:  100.0 1.0 50.0 0.1\n",
      "failed:  100.0 1.0 50.0 1.0\n",
      "failed:  100.0 1.0 50.0 10.0\n",
      "failed:  100.0 1.0 50.0 50.0\n",
      "failed:  100.0 1.0 50.0 100.0\n",
      "failed:  100.0 1.0 100.0 0.1\n",
      "failed:  100.0 1.0 100.0 1.0\n",
      "failed:  100.0 1.0 100.0 10.0\n",
      "failed:  100.0 1.0 100.0 50.0\n",
      "failed:  100.0 1.0 100.0 100.0\n",
      "failed:  100.0 10.0 0.1 0.1\n",
      "failed:  100.0 10.0 0.1 1.0\n",
      "failed:  100.0 10.0 0.1 10.0\n",
      "failed:  100.0 10.0 0.1 50.0\n",
      "failed:  100.0 10.0 0.1 100.0\n",
      "failed:  100.0 10.0 1.0 0.1\n",
      "failed:  100.0 10.0 1.0 1.0\n",
      "failed:  100.0 10.0 1.0 10.0\n",
      "failed:  100.0 10.0 1.0 50.0\n",
      "failed:  100.0 10.0 1.0 100.0\n",
      "failed:  100.0 10.0 10.0 0.1\n",
      "failed:  100.0 10.0 10.0 1.0\n",
      "failed:  100.0 10.0 10.0 10.0\n",
      "failed:  100.0 10.0 10.0 50.0\n",
      "failed:  100.0 10.0 10.0 100.0\n",
      "failed:  100.0 10.0 50.0 0.1\n",
      "failed:  100.0 10.0 50.0 1.0\n",
      "failed:  100.0 10.0 50.0 10.0\n",
      "failed:  100.0 10.0 50.0 50.0\n",
      "failed:  100.0 10.0 50.0 100.0\n",
      "failed:  100.0 10.0 100.0 0.1\n",
      "failed:  100.0 10.0 100.0 1.0\n",
      "failed:  100.0 10.0 100.0 10.0\n",
      "failed:  100.0 10.0 100.0 50.0\n",
      "failed:  100.0 10.0 100.0 100.0\n",
      "failed:  100.0 50.0 0.1 0.1\n",
      "failed:  100.0 50.0 0.1 1.0\n",
      "failed:  100.0 50.0 0.1 10.0\n",
      "failed:  100.0 50.0 0.1 50.0\n",
      "failed:  100.0 50.0 0.1 100.0\n",
      "failed:  100.0 50.0 1.0 0.1\n",
      "failed:  100.0 50.0 1.0 1.0\n",
      "failed:  100.0 50.0 1.0 10.0\n",
      "failed:  100.0 50.0 1.0 50.0\n",
      "failed:  100.0 50.0 1.0 100.0\n",
      "failed:  100.0 50.0 10.0 0.1\n",
      "failed:  100.0 50.0 10.0 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed:  100.0 50.0 10.0 10.0\n",
      "failed:  100.0 50.0 10.0 50.0\n",
      "failed:  100.0 50.0 10.0 100.0\n",
      "failed:  100.0 50.0 50.0 0.1\n",
      "failed:  100.0 50.0 50.0 1.0\n",
      "failed:  100.0 50.0 50.0 10.0\n",
      "failed:  100.0 50.0 50.0 50.0\n",
      "failed:  100.0 50.0 50.0 100.0\n",
      "failed:  100.0 50.0 100.0 0.1\n",
      "failed:  100.0 50.0 100.0 1.0\n",
      "failed:  100.0 50.0 100.0 10.0\n",
      "failed:  100.0 50.0 100.0 50.0\n",
      "failed:  100.0 50.0 100.0 100.0\n",
      "failed:  100.0 100.0 0.1 0.1\n",
      "failed:  100.0 100.0 0.1 1.0\n",
      "failed:  100.0 100.0 0.1 10.0\n",
      "failed:  100.0 100.0 0.1 50.0\n",
      "failed:  100.0 100.0 0.1 100.0\n",
      "failed:  100.0 100.0 1.0 0.1\n",
      "failed:  100.0 100.0 1.0 1.0\n",
      "failed:  100.0 100.0 1.0 10.0\n",
      "failed:  100.0 100.0 1.0 50.0\n",
      "failed:  100.0 100.0 1.0 100.0\n",
      "failed:  100.0 100.0 10.0 0.1\n",
      "failed:  100.0 100.0 10.0 1.0\n",
      "failed:  100.0 100.0 10.0 10.0\n",
      "failed:  100.0 100.0 10.0 50.0\n",
      "failed:  100.0 100.0 10.0 100.0\n",
      "failed:  100.0 100.0 50.0 0.1\n",
      "failed:  100.0 100.0 50.0 1.0\n",
      "failed:  100.0 100.0 50.0 10.0\n",
      "failed:  100.0 100.0 50.0 50.0\n",
      "failed:  100.0 100.0 50.0 100.0\n",
      "failed:  100.0 100.0 100.0 0.1\n",
      "failed:  100.0 100.0 100.0 1.0\n",
      "failed:  100.0 100.0 100.0 10.0\n",
      "failed:  100.0 100.0 100.0 50.0\n",
      "failed:  100.0 100.0 100.0 100.0\n"
     ]
    }
   ],
   "source": [
    "try_list = [0.1, 1.0, 10.0, 50.0, 100.0]\n",
    "for alpha in try_list:\n",
    "    for beta in try_list:\n",
    "        for gamma in try_list:\n",
    "            for omega in try_list:\n",
    "                try:\n",
    "                    model = RNN(n_inputs, n_hiddens, n_outputs, n_batch, seqlen)\n",
    "                    steps = 0\n",
    "                    for (data, labels) in train_gen:\n",
    "                        if steps >= train_steps:\n",
    "                            break\n",
    "                        model.warming(data, labels, alpha, beta, gamma, omega, warm_epochs)\n",
    "                        steps += 1\n",
    "                    print (\"------ success ------\",alpha, beta, gamma, omega)\n",
    "                except:\n",
    "                    print (\"failed: \", alpha, beta, gamma, omega)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-c30a0991a3b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_gen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtest_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-f45230c49664>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, inputs, labels)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;31m# (OUTPUT_DIM, N_BATCH)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubtract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/kaggle/salt/venv/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m     \"\"\"\n\u001b[0;32m--> 501\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_loss = []\n",
    "for (data, labels) in test_gen:\n",
    "    test_loss.append(model.evaluate(data, labels))\n",
    "print (sum(test_loss)/float(len(test_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1.0\n",
    "beta = 0.1 \n",
    "gamma = 0.1 \n",
    "omega = 0.1\n",
    "norm = 1.0\n",
    "\n",
    "warm_epochs = 5\n",
    "train_epochs = 5\n",
    "\n",
    "model = RNN(n_inputs, n_hiddens, n_outputs, n_batch, seqlen, norm)\n",
    "steps = 0\n",
    "for (data, labels) in train_gen:\n",
    "    if steps >= train_steps:\n",
    "        break\n",
    "    model.warming(data, labels, alpha, beta, gamma, omega, warm_epochs)\n",
    "    steps += 1\n",
    "\n",
    "for e in range(train_epochs):\n",
    "    steps = 0\n",
    "    for (data, labels) in train_gen:\n",
    "        if steps >= train_steps:\n",
    "            break\n",
    "        loss = model.fit(data, labels, alpha, beta, gamma, omega, val_gen)\n",
    "        print (\"epoch: %d, val loss: %.3f\"%(e, loss))\n",
    "        steps += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Test Result ---------\n",
      "seqlen: 12, loss: 0.9542\n",
      "--------- Test Result ---------\n",
      "seqlen: 24, loss: 0.0000\n",
      "--------- Test Result ---------\n",
      "seqlen: 48, loss: 0.7722\n",
      "--------- Test Result ---------\n",
      "seqlen: 96, loss: 0.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-137-628d18644e4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mtest_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mtest_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-134-831cb55beb4d>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, inputs, labels)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;31m# inputs: (input_dim, seq_len, N_BATCH)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;31m# labels: (output_dim, N_BATCH)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;31m# (OUTPUT_DIM, N_BATCH)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-134-831cb55beb4d>\u001b[0m in \u001b[0;36mfeed_forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m#(INPUT_DIM, N_BATCH)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-fd480821786d>\u001b[0m in \u001b[0;36mrelu\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for seqlen in [6, 12, 24, 48, 96, 192]:\n",
    "    #preparing the training, validation, and test generators\n",
    "    #seqlen = lookback//step\n",
    "    step = 6\n",
    "    lookback = seqlen * step\n",
    "    delay = 72  #from the past 10 days to predict the next day\n",
    "    batch_size = 128\n",
    "\n",
    "    #only training data need to randomly select batch\n",
    "    train_gen = generator(float_data, \n",
    "                          lookback=lookback,\n",
    "                          delay=delay,\n",
    "                          min_index=0,\n",
    "                          max_index=200000,\n",
    "                          shuffle=True,\n",
    "                          step=step,\n",
    "                          batch_size=batch_size)\n",
    "\n",
    "    val_gen = generator(float_data,\n",
    "                        lookback=lookback,\n",
    "                        delay=delay,\n",
    "                        min_index=200001,\n",
    "                        max_index=300000,\n",
    "                        step=step,\n",
    "                        batch_size=batch_size)\n",
    "\n",
    "    test_gen = generator(float_data,\n",
    "                         lookback=lookback,\n",
    "                         delay=delay,\n",
    "                         min_index=300001,\n",
    "                         max_index=None,\n",
    "                         step=step,\n",
    "                         batch_size=batch_size)\n",
    "\n",
    "    train_steps = (200001 - lookback) // batch_size\n",
    "    val_steps = (300000 - 200001 - lookback) // batch_size\n",
    "    test_steps = (len(float_data) - 300001 - lookback) // batch_size\n",
    "    \n",
    "    alpha = 10.0\n",
    "    beta = 0.1 \n",
    "    gamma = 1.0 \n",
    "    omega = 0.1\n",
    "    norm = 5.0\n",
    "\n",
    "    warm_epochs = 3\n",
    "    train_epochs = 10\n",
    "\n",
    "    model = RNN(n_inputs, n_hiddens, n_outputs, n_batch, seqlen, norm)\n",
    "    steps = 0\n",
    "    try:\n",
    "        for (data, labels) in train_gen:\n",
    "            if steps >= train_steps:\n",
    "                break\n",
    "            model.warming(data, labels, alpha, beta, gamma, omega, warm_epochs)\n",
    "            steps += 1\n",
    "\n",
    "        for e in range(train_epochs):\n",
    "            steps = 0\n",
    "            for (data, labels) in train_gen:\n",
    "                if steps >= train_steps:\n",
    "                    break\n",
    "                loss = model.fit(data, labels, alpha, beta, gamma, omega, val_gen)\n",
    "#                 print (\"epoch: %d, val loss: %.3f\"%(e, loss))\n",
    "                steps += 1\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    steps = 0\n",
    "    test_loss = []\n",
    "    for (data, labels) in test_gen:\n",
    "        if steps >= test_steps:\n",
    "            break\n",
    "        loss = model.evaluate(data, labels)\n",
    "        if loss < 1.0:\n",
    "            test_loss.append(loss)\n",
    "        steps += 1\n",
    "    print (\"--------- Test Result ---------\")\n",
    "    if len(test_loss) == 0:\n",
    "        test_loss += [0]\n",
    "    print (\"seqlen: {}, loss: {:.4f}\".format(seqlen, sum(test_loss)/float(len(test_loss))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch:  0\n",
      "Train epoch:  1\n",
      "Train epoch:  2\n",
      "Train epoch:  3\n",
      "Train epoch:  4\n",
      "--------- Test Result ---------\n",
      "seqlen: 24, loss: 0.6450\n",
      "Train epoch:  0\n",
      "Train epoch:  1\n",
      "Train epoch:  2\n",
      "--------- Test Result ---------\n",
      "seqlen: 96, loss: 0.0000\n",
      "--------- Test Result ---------\n",
      "seqlen: 192, loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "for seqlen in [24, 96, 192]:\n",
    "    #preparing the training, validation, and test generators\n",
    "    #seqlen = lookback//step\n",
    "    step = 6\n",
    "    lookback = seqlen * step\n",
    "    delay = 72  #from the past 10 days to predict the next day\n",
    "    batch_size = 128\n",
    "\n",
    "    #only training data need to randomly select batch\n",
    "    train_gen = generator(float_data, \n",
    "                          lookback=lookback,\n",
    "                          delay=delay,\n",
    "                          min_index=0,\n",
    "                          max_index=200000,\n",
    "                          shuffle=True,\n",
    "                          step=step,\n",
    "                          batch_size=batch_size)\n",
    "\n",
    "    val_gen = generator(float_data,\n",
    "                        lookback=lookback,\n",
    "                        delay=delay,\n",
    "                        min_index=200001,\n",
    "                        max_index=300000,\n",
    "                        step=step,\n",
    "                        batch_size=batch_size)\n",
    "\n",
    "    test_gen = generator(float_data,\n",
    "                         lookback=lookback,\n",
    "                         delay=delay,\n",
    "                         min_index=300001,\n",
    "                         max_index=None,\n",
    "                         step=step,\n",
    "                         batch_size=batch_size)\n",
    "\n",
    "    train_steps = (200001 - lookback) // batch_size\n",
    "    val_steps = (300000 - 200001 - lookback) // batch_size\n",
    "    test_steps = (len(float_data) - 300001 - lookback) // batch_size\n",
    "    \n",
    "    alpha = 10.0\n",
    "    beta = 0.1 \n",
    "    gamma = 1.0 \n",
    "    omega = 0.1\n",
    "    norm = 5.0\n",
    "\n",
    "    warm_epochs = 2\n",
    "    train_epochs = 5\n",
    "\n",
    "    model = RNN(n_inputs, n_hiddens, n_outputs, n_batch, seqlen, norm)\n",
    "    steps = 0\n",
    "    try:\n",
    "        for (data, labels) in train_gen:\n",
    "            if steps >= train_steps:\n",
    "                break\n",
    "            model.warming(data, labels, alpha, beta, gamma, omega, warm_epochs)\n",
    "            steps += 1\n",
    "\n",
    "        for e in range(train_epochs):\n",
    "            print (\"Train epoch: \", e)\n",
    "            steps = 0\n",
    "            for (data, labels) in train_gen:\n",
    "                if steps >= train_steps:\n",
    "                    break\n",
    "                model.fit(data, labels, alpha, beta, gamma, omega, val_gen)\n",
    "#                 print (\"epoch: %d, val loss: %.3f\"%(e, loss))\n",
    "                steps += 1\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    steps = 0\n",
    "    test_loss = []\n",
    "    for (data, labels) in test_gen:\n",
    "        if steps >= test_steps:\n",
    "            break\n",
    "        loss = model.evaluate(data, labels)\n",
    "        if loss < 1.0:\n",
    "            test_loss.append(loss)\n",
    "        steps += 1\n",
    "    print (\"--------- Test Result ---------\")\n",
    "    if len(test_loss) == 0:\n",
    "        test_loss += [0]\n",
    "    print (\"seqlen: {}, loss: {:.4f}\".format(seqlen, sum(test_loss)/float(len(test_loss))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seqlen: 6, loss: 0.7530"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
